{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Image Captioning using ResNet-152 and LSTM**\n",
    "\n",
    "## **Part 1: Encoding Images Using a Convolutional Neural Network (CNN)**\n",
    "\n",
    "The first step in the project involves encoding images into feature representations using a **pretrained Convolutional Neural Network (CNN)**. The CNN serves as an **encoder**, transforming high-dimensional image data into a lower-dimensional feature vector that captures essential visual information.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Select a Pretrained CNN**: A state-of-the-art model (ResNet-152) is used as the backbone for feature extraction.\n",
    "2. **Remove Fully Connected Layers**: Only the convolutional base of the CNN is retained, removing the final classification layers to obtain a **2048-dimensional feature vector**.\n",
    "3. **Extract Feature Vectors**: Each image is passed through the CNN, and the output of the final convolutional layer (or a global pooling layer) is taken as the image representation.\n",
    "4. **Store Feature Representations**: The extracted feature vectors are saved for efficient retrieval during training.\n",
    "\n",
    "This process enables the transformation of images into meaningful numerical embeddings that can be fed into the sequence modeling component of the system.\n",
    "\n",
    "## **Dataset: Flickr8k**\n",
    "- Contains **8,000 images**, each annotated with **5 captions**.\n",
    "- Preprocessing includes **resizing, cropping, normalization**, and **tokenization**.\n",
    "\n",
    "## **Technologies & Libraries**\n",
    "- **Deep Learning Framework:** PyTorch and Tensorflow\n",
    "- **Pretrained Model:** ResNet-152\n",
    "- **Text Processing:** custom tokenizer\n",
    "- **Dataset:** Flickr8k\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If necessary, download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k/Flickr8k_Dataset.zip\n",
    "# !unzip Flickr8k_Dataset.zip\n",
    "# !git clone https://github.com/ysbecca/flickr8k-custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from Pipeline.data_retrieving.Image_Caption_data_retriever import Image_Caption_data_retriever\n",
    "from Pipeline.preprocessing.Image_Caption_preprocessing import Image_Caption_preprocessing\n",
    "from Pipeline.modelling.dataloader.Image_Local_Dataloader import Image_Local_DataLoader\n",
    "from Pipeline.modelling.models.EncoderCNN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create an instance to the data retriever for the images and for the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_ID</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_ID  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 35445 training examples in the training set\n"
     ]
    }
   ],
   "source": [
    "training_data_retriever = Image_Caption_data_retriever()\n",
    "\n",
    "training_data_retriever.retrieve_data('./data/flickr8k-custom/captions/Flickr8k_train.token.txt')\n",
    "\n",
    "# print the head\n",
    "display(training_data_retriever.get_data().head())\n",
    "\n",
    "print(f\"there are {len(training_data_retriever.get_data())} training examples in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean and tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing\n",
    "\n",
    "# the preprocessing of the images is done in the data loader\n",
    "input_preprocessing_params = {}\n",
    "\n",
    "# preprocess the captions by removing the undesired characters and by tokenizing\n",
    "output_preprocessing_params = {\n",
    "    'lower_case':True,\n",
    "    'remove_punctuation':True,\n",
    "    'remove_stopwords':True,\n",
    "    'remove_digits':True,\n",
    "    'tokenizer':Custom_Tokenizer\n",
    "}\n",
    "\n",
    "preprocessor = Image_Caption_preprocessing(input_preprocessing_params, output_preprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# preprocess the captions\n",
    "list_of_captions = preprocessor.preprocess_output_data(training_data_retriever.get_data()['caption'])\n",
    "\n",
    "# save the captions\n",
    "with open(\"captions.pkl\", \"wb\") as file:\n",
    "    # Serialize the list and save it to the file\n",
    "    pickle.dump(list_of_captions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer to use it later in the decoder\n",
    "with open(\"custom_tokenizer.pkl\", \"wb\") as file:\n",
    "    # Serialize the list and save it to the file\n",
    "    pickle.dump(preprocessor.get_output_preprocessing_params()['tokenizer'], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the local path to the images \n",
    "local_path = './data/Flicker8k_Dataset/'\n",
    "training_data_retriever.get_data()['image_ID'] = training_data_retriever.get_data()['image_ID'].map(lambda x: local_path + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### create a dataloader for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Preprocessing to be apply to the images to be fed to the resnet (reference PyTorch doc)\n",
    "resenet_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the IDs inside the list are repeated 5 time, because for each image there are 5 captions. \n",
    "# Instead of computing the same feature 5 times, I will do it only once and then re-copy the result 5 times\n",
    "image_dataloader = Image_Local_DataLoader(\n",
    "    x=training_data_retriever.get_data()['image_ID'].to_list()[::5],\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    image_preprocessing_fn = resenet_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### create an instance of the EncoderCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet-152 and replace top fc layer\n",
    "encoder_net = EncoderCNN()\n",
    "\n",
    "# move the network to the GPU\n",
    "encoder_net = encoder_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of the model: torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Manually pass a dummy input to check the output shape of the EncodernCNN\n",
    "dummy_input = torch.randn(1,3,224,224).to(device) # random tensor of shape (1,3,224,224) -> 1 batch of 3 channels of size 224x224 (shape of the image accepted by the model)\n",
    "print(f'Output shape of the model: {encoder_net(dummy_input).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### encoder the images, using the EncoderCNN and the dataloader just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty tensor to put the features inside\n",
    "N_samples = image_dataloader.__len__()*5 # remember that each feature must repeat 5 times\n",
    "feature_lenght = 2048  # I know that the output will be of lenght 2048 because I inspected the architecture of the model and verified this value\n",
    "features = torch.empty((N_samples,feature_lenght), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7089/7089 [02:34<00:00, 45.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# iterate over the whole dataset\n",
    "start_idx = 0\n",
    "\n",
    "for inputs in tqdm(image_dataloader):\n",
    "    # convert from tf to torch\n",
    "    inputs = torch.tensor(inputs.numpy(), dtype=torch.float32)\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    batch_size = inputs.shape[0]\n",
    "    \n",
    "    # run the encoding \n",
    "    feature = encoder_net(inputs)\n",
    "    \n",
    "    # re-copy each feature five times\n",
    "    feature_copied_five_times = torch.empty((5*batch_size,feature_lenght), device=device)\n",
    "    \n",
    "    i = 0\n",
    "    for f in feature:\n",
    "        feature_copied_five_times[i:i+5] = f\n",
    "        i+=5\n",
    "          \n",
    "    # Store features in the tensor\n",
    "    features[start_idx : (start_idx + 5*batch_size)] = feature_copied_five_times\n",
    "\n",
    "    # Update index\n",
    "    start_idx += 5*batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the features locally\n",
    "torch.save(features.to('cpu'),'data/encoded_images_features.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
